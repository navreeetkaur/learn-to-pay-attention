{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = [256,256,3]\n",
    "NUM_CLASSES = 10\n",
    "PERC_VALID = 0.7\n",
    "NUM_TRAIN = 20000\n",
    "NUM_TEST = 2000\n",
    "\n",
    "# training hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "RMSPROP_DECAY = 0.9     \n",
    "RMSPROP_EPSILON = 1.0              \n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "DISPLAY_STEP = 5 #10\n",
    "VALIDATION_STEP = 100 #1000\n",
    "SAVE_STEP = 50 #100\n",
    "CKPT_PATH = './ckpt_concat_cp'\n",
    "CKPT_PREFIX = os.path.join(CKPT_PATH, 'ckpt')\n",
    "SUMMARY_PATH = './summary_concat_cp'\n",
    "\n",
    "# net architecture hyperparamaters\n",
    "LAMBDA = 5e-4 #for weight decay\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# test hyper parameters\n",
    "K_PATCHES = 5\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = (X_train[:NUM_TRAIN], Y_train[:NUM_TRAIN]), (X_test[:NUM_TEST], Y_test[:NUM_TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(x):\n",
    "    new_x = []\n",
    "    for i in range(len(x)):\n",
    "        new_x.append(zoom(x[i], (8.0,8.0,1.0)))\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = scale_dataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = scale_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = tf.convert_to_tensor(x_train, np.float32)\n",
    "# x_test = tf.convert_to_tensor(x_test, np.float32)\n",
    "y_train = tf.convert_to_tensor(Y_train, np.float32)\n",
    "# y_test = tf.convert_to_tensor(Y_test, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 150)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = x_train.numpy().shape[0]\n",
    "train_len = int(total*(1-PERC_VALID))\n",
    "val_len = total-train_len\n",
    "train_len, val_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train[:train_len], y_train[:train_len]))\n",
    "dataset = dataset.shuffle(100).batch(BATCH_SIZE)\n",
    "data_it = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "testset = testset.shuffle(10).batch(1)\n",
    "test_it = testset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset = tf.data.Dataset.from_tensor_slices((x_train[train_len:], y_train[train_len:]))\n",
    "valset = valset.shuffle(100).batch(BATCH_SIZE)\n",
    "val_it = valset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(tfe.Network):\n",
    "\n",
    "    def __init__(self, training):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.training = training\n",
    "\n",
    "        # convolutional layers\n",
    "\n",
    "        conv_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "        self.conv1 = self.track_layer(tf.layers.Conv2D(96, 11, 4, 'SAME', \n",
    "                                                        activation=tf.nn.relu, \n",
    "                                                        kernel_initializer=conv_init))\n",
    "        self.pool1 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        self.conv2 = self.track_layer(tf.layers.Conv2D(256, 5, 1, 'SAME', \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=conv_init))\n",
    "        self.pool2 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        self.conv3 = self.track_layer(tf.layers.Conv2D(384, 3, 1, 'SAME', \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=conv_init))\n",
    "\n",
    "        self.conv4 = self.track_layer(tf.layers.Conv2D(384, 3, 1, 'SAME', \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=conv_init))\n",
    "\n",
    "        self.conv5 = self.track_layer(tf.layers.Conv2D(256, 3, 1, 'SAME', \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=conv_init))\n",
    "        self.pool5 = self.track_layer(tf.layers.MaxPooling2D(3, 2, 'VALID'))\n",
    "\n",
    "        # fully connected layers\n",
    "\n",
    "        \n",
    "        fc_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.fc1 = self.track_layer(tf.layers.Dense(512, \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=fc_init))\n",
    "        self.drop1 = self.track_layer(tf.layers.Dropout(DROPOUT))\n",
    "        \n",
    "        self.perceptron_u = self.track_layer(tf.layers.Dense(1, \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=fc_init))\n",
    "\n",
    "        self.att1 = self.track_layer(tf.layers.Dense(512, \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=fc_init))\n",
    "        self.att2 = self.track_layer(tf.layers.Dense(512, \n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=fc_init))\n",
    "        \n",
    "        self.out = self.track_layer(tf.layers.Dense(NUM_CLASSES,\n",
    "                                                        kernel_initializer=fc_init))\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\" Function that executes the model \"\"\"\n",
    "        output = self.conv1(x)\n",
    "        print(f'Conv1: {output.numpy().shape}')\n",
    "        output = tf.nn.lrn(output, depth_radius=2, bias=1.0, alpha=2e-05, beta=0.75)\n",
    "        output = self.pool1(output)\n",
    "        print(f'Pool1: {output.numpy().shape}')\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        print(f'Conv2: {output.numpy().shape}')\n",
    "        output = tf.nn.lrn(output, depth_radius=2, bias=1.0, alpha=2e-05, beta=0.75)\n",
    "        output = self.pool2(output)\n",
    "        print(f'Pool2: {output.numpy().shape}')\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        print(f'Conv3: {output.numpy().shape}')\n",
    "\n",
    "        output = self.conv4(output)\n",
    "        print(f'Conv4: {output.numpy().shape}')\n",
    "        output_conv4 = output \n",
    "\n",
    "        output = self.conv5(output)\n",
    "        print(f'Conv5: {output.numpy().shape}')\n",
    "        output_conv5 = output \n",
    "        output = self.pool5(output)\n",
    "        print(f'Pool5: {output.numpy().shape}')\n",
    "\n",
    "        output = tf.layers.flatten(output)\n",
    "        print(f'Flatten1: {output.numpy().shape}')\n",
    "\n",
    "        output = self.fc1(output)\n",
    "        print(f'FC1: {output.numpy().shape}')\n",
    "        if self.training:\n",
    "            output = self.drop1(output)\n",
    "        \n",
    "        g = tf.layers.flatten(output)\n",
    "        g = tf.reshape(g, [g.numpy().shape[0],1,g.numpy().shape[1]])\n",
    "        print(f'G: {g.numpy().shape}')\n",
    "        \n",
    "        ##### Attention 1 - conv 4 #####\n",
    "        output_att1 = self.att1(output_conv4)\n",
    "        # reshaping L to match G's dimension to compute compatibility scores\n",
    "        output_att1_shapes = output_att1.numpy().shape\n",
    "        output_att1 = tf.reshape(output_att1,  [output_att1_shapes[0], output_att1_shapes[1]*output_att1_shapes[2], output_att1_shapes[3],])\n",
    "        print(f'Att1: {output_att1.numpy().shape}')\n",
    "        # compatibility score\n",
    "#         compat_att1 = tf.matmul(g,output_att1)\n",
    "        compat_att1 = self.perceptron_u(tf.add(g,output_att1))\n",
    "        print(f'compatability scores 1: {compat_att1.numpy().shape}')\n",
    "        # normalising the compatibiltiy scores by softmax\n",
    "        att_att1 = tf.nn.softmax(compat_att1)\n",
    "        att_att1 = tf.reshape(att_att1, [att_att1.numpy().shape[0], att_att1.numpy().shape[2], att_att1.numpy().shape[1]])\n",
    "        print(f'A1: {att_att1.numpy().shape}')\n",
    "        # reshape L1 for computing g1\n",
    "        output_conv4_shapes = output_conv4.numpy().shape\n",
    "        output_conv4 = tf.reshape(output_conv4, [output_conv4_shapes[0], output_conv4_shapes[1]*output_conv4_shapes[2], output_conv4_shapes[3]])\n",
    "        print(f'L1: {output_conv4.numpy().shape}')\n",
    "        # computing g1\n",
    "        g1 = tf.matmul(att_att1, output_conv4)\n",
    "        g1 = tf.layers.flatten(g1)\n",
    "        print(f'--- G1: {g1.numpy().shape} --- \\n')\n",
    "        \n",
    "        ##### Attention 2 -  conv 5 #####\n",
    "        output_att2 = self.att2(output_conv5)\n",
    "        print(f'Att2: {output_att2.numpy().shape}')\n",
    "        # reshaping L to match G's dimension to compute compatibility scores\n",
    "        output_att2_shapes = output_att2.numpy().shape\n",
    "        output_att2 = tf.reshape(output_att2, [output_att2_shapes[0],output_att2_shapes[1]*output_att2_shapes[2], output_att2_shapes[3]])\n",
    "        print(f'Att2: {output_att2.numpy().shape}')\n",
    "        # compatibility scores\n",
    "        #compat_att2 = tf.matmul(g,output_att2)\n",
    "        compat_att2 = self.perceptron_u(tf.add(g,output_att2))\n",
    "        print(f'compatability scores 2: {compat_att2.numpy().shape}')\n",
    "        # normalising the compatibiltiy scores by softmax\n",
    "        att_att2 = tf.nn.softmax(compat_att2)\n",
    "        att_att2 = tf.reshape(att_att1, [att_att2.numpy().shape[0], att_att2.numpy().shape[2], att_att2.numpy().shape[1]])\n",
    "        print(f'A2: {att_att2.numpy().shape}')\n",
    "        # reshape L2 for computing g2\n",
    "        output_conv5_shapes = output_conv5.numpy().shape\n",
    "        output_conv5 = tf.reshape(output_conv5, [output_conv5_shapes[0], output_conv5_shapes[1]*output_conv5_shapes[2], output_conv5_shapes[3]])\n",
    "        print(f'L2: {output_conv5.numpy().shape}')\n",
    "        # computing g1\n",
    "        g2 = tf.matmul(att_att2, output_conv5)\n",
    "        g2 = tf.layers.flatten(g2)\n",
    "        print(f'--- G2: {g2.numpy().shape} ---\\n')\n",
    "        # Computing final g' by concatenation\n",
    "        g_ = tf.concat([g1,g2], axis=1)\n",
    "        print(f'G dash: {g_.numpy().shape}')      \n",
    "        output = self.out(g_)\n",
    "        print(f'Logits: {output.numpy().shape}')  \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:** tfe.Network is deprecated and will be removed in a future version.\n",
      "\n",
      "Please inherit from `tf.keras.Model`, and see its documentation for details. `tf.keras.Model` should be a drop-in replacement for `tfe.Network` in most cases, but note that `track_layer` is no longer necessary or supported. Instead, `Layer` instances are tracked on attribute assignment (see the section of `tf.keras.Model`'s documentation on subclassing). Since the output of `track_layer` is often assigned to an attribute anyway, most code can be ported by simply removing the `track_layer` calls.\n",
      "\n",
      "`tf.keras.Model` works with all TensorFlow `Layer` instances, including those from `tf.layers`, but switching to the `tf.keras.layers` versions along with the migration to `tf.keras.Model` is recommended, since it will preserve variable names. Feel free to import it with an alias to avoid excess typing :).\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1: (5, 64, 64, 96)\n",
      "Pool1: (5, 31, 31, 96)\n",
      "Conv2: (5, 31, 31, 256)\n",
      "Pool2: (5, 15, 15, 256)\n",
      "Conv3: (5, 15, 15, 384)\n",
      "Conv4: (5, 15, 15, 384)\n",
      "Conv5: (5, 15, 15, 256)\n",
      "Pool5: (5, 7, 7, 256)\n",
      "Flatten1: (5, 12544)\n",
      "FC1: (5, 512)\n",
      "G: (5, 1, 512)\n",
      "Att1: (5, 225, 512)\n",
      "compatability scores 1: (5, 225, 1)\n",
      "A1: (5, 1, 225)\n",
      "L1: (5, 225, 384)\n",
      "--- G1: (5, 384) --- \n",
      "\n",
      "Att2: (5, 15, 15, 512)\n",
      "Att2: (5, 225, 512)\n",
      "compatability scores 2: (5, 225, 1)\n",
      "A2: (5, 1, 225)\n",
      "L2: (5, 225, 256)\n",
      "--- G2: (5, 256) ---\n",
      "\n",
      "G dash: (5, 640)\n",
      "Logits: (5, 10)\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation Warning: create_summary_file_writer was renamed to create_file_writer\n"
     ]
    }
   ],
   "source": [
    "writer = tf.contrib.summary.create_summary_file_writer(SUMMARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=LEARNING_RATE, momentum=MOMENTUM)\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE, RMSPROP_DECAY,momentum=MOMENTUM,epsilon=RMSPROP_EPSILON)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "# opt = tf.train.GradientDescent(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(model, mode, x, y):\n",
    "    logits = model(x)\n",
    "    y = list(y.numpy().reshape(y.numpy().shape[0],))\n",
    "    y = tf.one_hot(y, NUM_CLASSES)\n",
    "    loss_value = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits)\n",
    "#     loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y), name=\"cross_entropy_loss\")\n",
    "    weight_decay = tf.reduce_sum(LAMBDA * tf.stack([tf.nn.l2_loss(v) for v in model.variables]))\n",
    "\n",
    "    total_loss = loss_value + weight_decay\n",
    "\n",
    "    tf.contrib.summary.scalar(mode, '/loss', total_loss)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(model, mode, x, y):\n",
    "#     pred = tf.nn.softmax(model(x))\n",
    "\n",
    "#     accuracy_value = tf.reduce_sum(\n",
    "#                 tf.cast(\n",
    "#                     tf.equal(\n",
    "#                         tf.argmax(pred, axis=1, output_type=tf.int64),\n",
    "#                         tf.argmax(y, axis=1, output_type=tf.int64)\n",
    "#                     ),\n",
    "#                     dtype=tf.float32\n",
    "#                 ) \n",
    "#             ) / float(pred.shape[0].value)\n",
    "    pred = tf.cast(tf.argmax(tf.nn.softmax(model(x)),axis=1), dtype=tf.float32)\n",
    "    equality = tf.equal(pred, y)\n",
    "    accuracy_value = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "    tf.contrib.summary.scalar(mode, '/accuracy', accuracy_value)\n",
    "\n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_time(time):\n",
    "    \"\"\" It formats a datetime to print it\n",
    "        Args:\n",
    "            time: datetime\n",
    "        Returns:\n",
    "            a formatted string representing time\n",
    "    \"\"\"\n",
    "    m, s = divmod(time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    d, h = divmod(h, 24)\n",
    "    return ('{:02d}d {:02d}h {:02d}m {:02d}s').format(int(d), int(h), int(m), int(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_variables = (model.variables + optimizer.variables() + [global_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19\n",
      "Epoch: 000 Step/Batch: 019 Step mean time: 1039ms \n",
      "Loss: nan Training accuracy: 1.0000\n",
      "step: 20\n",
      "Epoch: 000 Step/Batch: 020 Step mean time: 1989ms \n",
      "Loss: nan Training accuracy: 1.0000\n",
      "Elapsed time: 00d 00h 01m 17s --- Loss: nan Validation accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-947980252a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSAVE_STEP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCKPT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'net.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variables saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_variables' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step_time = 0.0\n",
    "global_step = 0\n",
    "# with writer.as_default():\n",
    "#     with tf.contrib.summary.record_summaries_every_n_global_steps(DISPLAY_STEP):\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch_i,datum) in enumerate(data_it):\n",
    "        global_step = tf.train.get_global_step()\n",
    "#                 global_step = tf.train.get_or_create_global_step()\n",
    "        step = global_step.numpy() + 1\n",
    "#         global_step += 1\n",
    "        print(f'step: {step}')\n",
    "        step_start_time = int(round(time.time() * 1000))\n",
    "        optimizer.minimize(lambda: loss(model, 'train', datum[0], datum[1]), global_step=global_step)\n",
    "\n",
    "        step_end_time = int(round(time.time() * 1000))\n",
    "        step_time = step_time + step_end_time - step_start_time\n",
    "\n",
    "        if (step % DISPLAY_STEP) == 0:\n",
    "            l = loss(model, 'train', datum[0], datum[1])\n",
    "            a = accuracy(model, 'train', datum[0], datum[1]).numpy()\n",
    "            print ('Epoch: {:03d} Step/Batch: {:03d} Step mean time: {:04d}ms \\nLoss: {:.7f} Training accuracy: {:.4f}'.format(epoch, step, int(step_time / step), l, a))\n",
    "\n",
    "        if (step % VALIDATION_STEP) == 0:\n",
    "            val_images, val_labels = val_it.get_next()\n",
    "            l = loss(model, 'val', val_images, val_labels)\n",
    "            a = accuracy(model, 'val', val_images, val_labels).numpy()\n",
    "            int_time = time.time() - start_time\n",
    "            print ('Elapsed time: {} --- Loss: {:.7f} Validation accuracy: {:.4f}'.format(format_time(int_time), l, a))\n",
    "\n",
    "        if (step % SAVE_STEP) == 0:\n",
    "            tfe.Saver(all_variables).save(os.path.join(CKPT_PATH, 'net.ckpt'), global_step=global_step)\n",
    "            print('Variables saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ell-881-2018-deep-learning",
   "language": "python",
   "name": "ell-881-2018-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
